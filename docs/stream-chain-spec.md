# Stream-Chain Specification and Pseudocode

> Generated by SPARC Spec-Pseudocode Mode | 2026-02-26

## 1. Functional Requirements

### FR-1: Pipeline Configuration
The system SHALL store pipeline definitions in `.claude-flow/config.json` with the following schema:

```
config.json
├── projectName: string
├── version: string
└── streamChain
    ├── defaultTimeout: integer (seconds)
    ├── outputDir: string (relative path)
    └── pipelines: map<string, Pipeline>
        └── Pipeline
            ├── description: string
            ├── timeout: integer (overrides defaultTimeout)
            └── steps: array<Step>
                └── Step
                    ├── name: string (unique within pipeline)
                    └── prompt: string (agent instruction)
```

### FR-2: Pipeline Execution
The wrapper script SHALL:
- Accept a pipeline name as positional argument
- Extract prompts and timeout from config.json
- Invoke `npx claude-flow stream-chain run` with all prompts in order
- Capture exit code, duration, and output

### FR-3: Result Persistence
Each execution SHALL produce:
- A JSON summary file with run metadata
- A raw output log file with CLI stdout/stderr
- Both stored in `.claude-flow/memory/stream-chain/runs/`

### FR-4: Event Emission
Each execution SHALL emit a Loki event to `.loki/events/pending/` using the `task_complete` type, regardless of success or failure.

### FR-5: Operational Modes
The script SHALL support:
- `--dry-run`: Display prompts without execution
- `--bg`: Execute in background (forked process)
- `--verbose`: Display the full CLI command
- `--list`: Display all available pipelines

## 2. Non-Functional Requirements

| Requirement | Constraint |
|-------------|-----------|
| Shell compatibility | bash 3.2+ (macOS default) |
| Python dependency | python3 on PATH (stdlib only, no pip packages) |
| No git tracking | All output in gitignored directories |
| Idempotent | Multiple runs produce independent result files (timestamped) |
| Failure isolation | Pipeline failure does not corrupt state or block future runs |

## 3. Data Flow Pseudocode

### 3.1 Main Entry (`stream-chain-run.sh`)

```pseudocode
FUNCTION main(args):
    PROJECT_DIR = resolve(dirname(script) + "/..")
    CONFIG_FILE = PROJECT_DIR + "/.claude-flow/config.json"
    RUNS_DIR = PROJECT_DIR + "/.claude-flow/memory/stream-chain/runs"
    EVENTS_DIR = PROJECT_DIR + "/.loki/events/pending"

    # Phase 1: Parse arguments
    pipeline_name = NULL
    flags = {dry_run: false, background: false, verbose: false}

    FOR arg IN args:
        MATCH arg:
            "--dry-run"  -> flags.dry_run = true
            "--bg"       -> flags.background = true
            "--verbose"  -> flags.verbose = true
            "--list"     -> call list_pipelines(CONFIG_FILE); EXIT 0
            starts_with("-") -> ERROR "Unknown flag"; EXIT 1
            otherwise    -> pipeline_name = arg

    IF pipeline_name IS NULL:
        print_usage(); EXIT 1

    # Phase 2: Validate prerequisites
    IF NOT exists(CONFIG_FILE):
        ERROR "Config not found"; EXIT 1

    # Phase 3: Extract pipeline definition
    (timeout, prompts[]) = extract_pipeline(CONFIG_FILE, pipeline_name)
    # On error: prints available pipelines and exits 1

    step_count = length(prompts)
    PRINT "Pipeline: {pipeline_name} ({step_count} steps, {timeout}s/step)"

    # Phase 4: Handle dry run
    IF flags.dry_run:
        FOR i, prompt IN enumerate(prompts):
            PRINT "Step {i+1}: {truncate(prompt, 120)}..."
        PRINT "Would run: npx claude-flow stream-chain run <prompts> --timeout {timeout}"
        EXIT 0

    # Phase 5: Prepare execution context
    ensure_dirs(RUNS_DIR, EVENTS_DIR)
    timestamp = format_date("YYYYMMDD-HHMMSS")
    run_id = "{pipeline_name}-{timestamp}"
    result_file = RUNS_DIR + "/{run_id}.json"
    output_log = RUNS_DIR + "/{run_id}-output.log"

    # Phase 6: Build command
    cmd = ["npx", "claude-flow", "stream-chain", "run"]
    FOR prompt IN prompts:
        cmd.append(prompt)
    cmd.append("--timeout", timeout)

    IF flags.verbose:
        PRINT "Command: {cmd}"

    # Phase 7: Execute
    IF flags.background:
        PRINT "Running in background..."
        fork(run_pipeline(cmd, run_id, ...))
        PRINT "Background PID: {pid}"
    ELSE:
        run_pipeline(cmd, run_id, result_file, output_log, EVENTS_DIR, ...)
```

### 3.2 Pipeline Execution

```pseudocode
FUNCTION run_pipeline(cmd, run_id, result_file, output_log, events_dir, pipeline, step_count, timeout):
    start_time = now_epoch()

    # Execute with output capture
    exit_code = execute(cmd, stdout=output_log, stderr=output_log)

    end_time = now_epoch()
    duration = end_time - start_time
    status = "success" IF exit_code == 0 ELSE "failed"
    iso_timestamp = format_utc_iso()

    # Write result summary
    result = {
        runId: run_id,
        pipeline: pipeline,
        steps: step_count,
        timeout: timeout,
        exitCode: exit_code,
        durationSeconds: duration,
        status: status,
        timestamp: iso_timestamp,
        outputLog: output_log
    }
    write_json(result_file, result)
    PRINT result (formatted)

    # Emit Loki event (task field required by loki-event-sync.sh handle_task_complete)
    event = {
        type: "task_complete",
        task: "stream-chain/{status}: {pipeline} ({step_count} steps, {duration}s)",
        source: "stream-chain",
        pipeline: pipeline,
        runId: run_id,
        status: status,
        durationSeconds: duration,
        timestamp: iso_timestamp
    }
    event_file = events_dir + "/stream-chain-{run_id}.json"
    write_json(event_file, event)

    # Report outcome
    IF exit_code == 0:
        PRINT "Pipeline '{pipeline}' completed successfully in {duration}s"
    ELSE:
        PRINT_STDERR "Pipeline '{pipeline}' failed (exit {exit_code}) after {duration}s"

    RETURN exit_code
```

### 3.3 Config Extraction

```pseudocode
FUNCTION extract_pipeline(config_file, pipeline_name) -> (timeout, prompts[]):
    config = parse_json(read_file(config_file))
    pipelines = config.streamChain.pipelines

    IF pipeline_name NOT IN pipelines:
        PRINT_STDERR "Pipeline not found. Available: {keys(pipelines)}"
        EXIT 1

    pipeline = pipelines[pipeline_name]
    timeout = pipeline.timeout ?? config.streamChain.defaultTimeout
    prompts = [step.prompt FOR step IN pipeline.steps]

    RETURN (timeout, prompts)
```

### 3.4 Pipeline Listing

```pseudocode
FUNCTION list_pipelines(config_file):
    config = parse_json(read_file(config_file))

    PRINT "Available pipelines:"
    FOR name, pipeline IN config.streamChain.pipelines:
        steps = length(pipeline.steps)
        timeout = pipeline.timeout ?? config.streamChain.defaultTimeout
        PRINT "  {name:16} {pipeline.description:52} ({steps} steps, {timeout}s/step)"
```

## 4. Pipeline Definitions

### 4.1 `test` Pipeline

```
Step 1 [backend-tests]:   Run .venv/bin/python -m pytest backend/tests/ -v --tb=short
                          Compare against baseline (1253 tests), flag regressions
Step 2 [frontend-tests]:  Run npx jest --ci --no-cache in frontend/
                          Compare against baseline (834 tests, 52 suites)
Step 3 [ml-tests]:        Run .venv/bin/python -m pytest ml/tests/ -v --tb=short
                          Compare against baseline (257 tests, 41 skipped)
Step 4 [gap-report]:      Analyze results from steps 1-3
                          Cross-reference endpoints/components/services with test files
                          Output prioritized gap list with suggested filenames
```

### 4.2 `security` Pipeline

```
Step 1 [owasp-audit]:     Audit for OWASP Top 10 (SQLi, XSS, auth bypass, CSRF)
                          Check parameterized queries, input sanitization, session validation
Step 2 [dependency-scan]: Run pip audit (Python) + npm audit (JavaScript)
                          Report CVEs with severity and remediation steps
Step 3 [secrets-scan]:    Scan for hardcoded keys (sk_live_, Bearer, password=)
                          Verify .env in .gitignore, SecretsManager usage, no client-side keys
Step 4 [adversarial-tests]: Run test_security_adversarial.py (42 tests)
                          Identify uncovered attack vectors
```

### 4.3 `refactor` Pipeline

```
Step 1 [complexity-scan]:    Find functions > 50 lines, cyclomatic complexity > 10
                             Find React components > 200 lines. Top 10 list
Step 2 [tech-debt-inventory]: TODO/FIXME/HACK, deprecated APIs, dead code, inconsistencies
                             Categorize by severity and effort
Step 3 [refactor-plan]:      Prioritize by impact/effort ratio
                             Group: quick wins (< 1h), medium (1-4h), major (> 4h)
```

### 4.4 `feature-tdd` Pipeline

```
Step 1 [spec]:            Technical specification from feature input
                          API endpoints, DB models (UUID PKs), frontend components
Step 2 [failing-tests]:   Write pytest + Jest tests that MUST fail
                          Follow function-scoped TestClient, mock_sqlalchemy_select patterns
Step 3 [implement]:       Write code to pass all failing tests
                          FastAPI routers, Pydantic models, services, React components
Step 4 [verify-refactor]: Full suite regression check
                          Code review for complexity, error handling, pattern consistency
```

### 4.5 `deploy-check` Pipeline

```
Step 1 [test-gate]:       All 3 suites must pass with zero failures
Step 2 [config-check]:    render.yaml env vars, Dockerfile, CSP/HSTS, Swagger disabled
Step 3 [migration-check]: Sequential numbering, conftest.py sync, no stale refs
Step 4 [prod-readiness]:  No .env in git, no debug code, /health, CORS, rate limiting
                          Output: go/no-go recommendation
```

### 4.6 `analysis` Pipeline

```
Step 1 [architecture-map]:    API routers + endpoints, services + deps, routes, integrations
Step 2 [dependency-analysis]: Import chains, circular deps, coupling, prop drilling
Step 3 [debt-report]:         Architectural risks, coupling hotspots, test debt, dep risks
                              Prioritized by business impact with remediation timeline
```

## 5. Error Handling Matrix

| Error | Source | Handling | User Impact |
|-------|--------|----------|-------------|
| Config file missing | Filesystem | Exit 1 with message | Script won't run |
| Invalid pipeline name | Config parsing | Exit 1 with available names | User retries with valid name |
| JSON parse error | Config file | Python exception, exit 1 | Fix config.json |
| claude-flow not installed | npx | Non-zero exit from CLI | Install claude-flow |
| Step timeout exceeded | claude-flow | CLI returns non-zero | Result shows "failed" |
| Loki events dir missing | Filesystem | `mkdir -p` in script | Auto-created |
| Background process crash | OS | Exit code captured in log | Check output log |
| Python3 not on PATH | System | Script fails at extraction | Install python3 |

## 6. Integration Points

```
                    ┌─────────────────────────────────┐
                    │     stream-chain-run.sh          │
                    └──────────┬──────────────────────┘
                               │
              ┌────────────────┼────────────────┐
              │                │                │
              v                v                v
    .claude-flow/        npx claude-flow    .loki/events/
    config.json          stream-chain       pending/
    (read-only)          run (execute)      (write)
                               │                │
                               v                v
                    .claude-flow/memory/   loki-event-sync.sh
                    stream-chain/runs/     (existing hook)
                    (write)                    │
                                    ┌──────────┼──────────┐
                                    v          v          v
                              GitHub Proj  Notion DB  Claude-flow
                              #4 sync      sync       memory persist
```

## 7. File Naming Conventions

| File | Pattern | Example |
|------|---------|---------|
| Result JSON | `<pipeline>-<YYYYMMDD-HHMMSS>.json` | `test-20260226-143022.json` |
| Output log | `<pipeline>-<YYYYMMDD-HHMMSS>-output.log` | `test-20260226-143022-output.log` |
| Loki event | `stream-chain-<pipeline>-<YYYYMMDD-HHMMSS>.json` | `stream-chain-test-20260226-143022.json` |

## 8. TDD Anchors

Tests for stream-chain should verify:

```pseudocode
TEST "list_pipelines returns all 6 pipelines":
    output = run("scripts/stream-chain-run.sh --list")
    ASSERT "test" IN output
    ASSERT "security" IN output
    ASSERT "refactor" IN output
    ASSERT "feature-tdd" IN output
    ASSERT "deploy-check" IN output
    ASSERT "analysis" IN output

TEST "dry_run prints prompts without executing":
    output = run("scripts/stream-chain-run.sh test --dry-run")
    ASSERT "DRY RUN" IN output
    ASSERT "Step 1" IN output
    ASSERT "Step 4" IN output
    ASSERT "Would run" IN output

TEST "invalid pipeline name shows error":
    result = run("scripts/stream-chain-run.sh nonexistent", expect_fail=true)
    ASSERT result.exit_code == 1
    ASSERT "not found" IN result.stderr
    ASSERT "Available:" IN result.stderr

TEST "missing args shows usage":
    result = run("scripts/stream-chain-run.sh", expect_fail=true)
    ASSERT result.exit_code == 1
    ASSERT "Usage:" IN result.stdout

TEST "config.json is valid JSON with required schema":
    config = parse_json(read(".claude-flow/config.json"))
    ASSERT "streamChain" IN config
    ASSERT "pipelines" IN config.streamChain
    ASSERT length(config.streamChain.pipelines) == 6
    FOR name, pipeline IN config.streamChain.pipelines:
        ASSERT "steps" IN pipeline
        ASSERT length(pipeline.steps) >= 3
        FOR step IN pipeline.steps:
            ASSERT "name" IN step
            ASSERT "prompt" IN step
```
